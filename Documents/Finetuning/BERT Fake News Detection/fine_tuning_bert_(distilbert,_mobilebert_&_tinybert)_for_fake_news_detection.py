# -*- coding: utf-8 -*-
"""Fine-tuning BERT (distilBERT, MobileBERT & TinyBERT) for Fake News Detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MYUE9g7OdvE1KtvQUU-m2bmRYvo_-Np5
"""

!pip install -U transformers
!pip install -U accelerate
!pip install -U datasets
!pip install -U bertviz
!pip install -U umap-learn
!pip install seaborn --upgrade
!pip install -U openpyxl

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
df = pd.read_excel("https://github.com/laxmimerit/All-CSV-ML-Data-Files-Download/raw/master/fake_news.xlsx")
df.head()

df.isnull().sum()
df.shape

df = df.dropna()

df.isnull().sum()

df.shape

df = df.drop_duplicates().reset_index(drop=True)

df.shape

df['label'].value_counts()

"""##Data Analysis"""

import matplotlib.pyplot as plt
label_counts = df['label'].value_counts(ascending=True)
plt.bar(label_counts.index, label_counts.values)
plt.xlabel('Label')
plt.ylabel('Count')
plt.title('Label Distribution')
plt.show()

# 1.5 tokens per word on average
df['title_tokens']=df['title'].apply(lambda x: len(x.split())*1.5)
df['text_tokens']=df['text'].apply(lambda x: len(x.split())*1.5)

df.head()

fig, ax = plt.subplots(1,2, figsize=(15,5))
ax[0].hist(df['title_tokens'], bins=50, color='red')
ax[0].set_title('Title Tokens Distribution')
ax[1].hist(df['text_tokens'], bins=50, color='blue')
ax[1].set_title('Text Tokens Distribution')
plt.show()

"""Note : Text Tokens length has been exceeding 512 tokens as we can see in the chart, So title tokens would be ideal fit for predictions as in 'text' column text would be truncated.

##Data Loader and Train Test Split
"""

from sklearn.model_selection import train_test_split
# 70% for training, 20% test, 10% validation

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])
test_df, val_df = train_test_split(test_df, test_size=1/3, random_state=42, stratify=test_df['label'])

df.shape, train_df.shape, val_df.shape, test_df.shape

from datasets import Dataset, DatasetDict

dataset = DatasetDict({
    'train': Dataset.from_pandas(train_df, preserve_index=False),
    'test': Dataset.from_pandas(test_df, preserve_index=False),
    'validation': Dataset.from_pandas(val_df, preserve_index=False)
})

dataset

"""## Data Tokenization"""

from transformers import AutoTokenizer

text = "LLM has powered NLP in a really transformed manner."

model_ckpt = "distilbert-base-uncased"
distilbert_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
distilbert_tokens = distilbert_tokenizer.tokenize(text)

model_ckpt = "google/mobilebert-uncased"
mobilebert_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
mobilebert_tokens = mobilebert_tokenizer.tokenize(text)

model_ckpt = "huawei-noah/TinyBERT_General_4L_312D"
tinybert_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
tinybert_tokens = tinybert_tokenizer.tokenize(text)

distilbert_tokenizer, mobilebert_tokenizer, tinybert_tokenizer

def tokenize(batch):
  temp = distilbert_tokenizer(batch['title'], truncation=True)
  return temp

print(tokenize(dataset['train'][0]))

encoded_dataset = dataset.map(tokenize, batch_size=None, batched=True)

"""##Model Buidling"""

from transformers import AutoModelForSequenceClassification, AutoConfig
import torch

label2id = {
    "Real":0,
    "Fake":1
}
id2label = {
    0:"Real",
    1:"Fake"
}

num_labels = len(label2id)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_ckpt = "distilbert-base-uncased"
config = AutoConfig.from_pretrained(model_ckpt, num_labels=num_labels, label2id=label2id, id2label=id2label)
model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config).to(device)

model.config.id2label

model.config

!pip install evaluate

import evaluate
import numpy as np

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
  predictions, labels = eval_pred
  predictions = np.argmax(predictions, axis=1)
  return accuracy.compute(predictions=predictions, references=labels)

"""## Model Trainng"""

from logging import disable
from transformers import TrainingArguments

batch_size = 32
training_dir = "train_dir"

training_args = TrainingArguments(
    output_dir=training_dir,
    num_train_epochs=2,
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    warmup_steps=500,
    weight_decay=0.01,
    disable_tqdm=False,
    report_to="none"
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    compute_metrics=compute_metrics,
    args=training_args,
    train_dataset=encoded_dataset['train'],
    eval_dataset=encoded_dataset['validation'],
    tokenizer=distilbert_tokenizer
)

trainer.train()

"""## Model Evaluation"""

preds_output = trainer.predict(encoded_dataset['test'])
preds_output.metrics

y_pred = np.argmax(preds_output.predictions, axis=1)
y_true = encoded_dataset['test']['label']

from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(y_true, y_pred, target_names=list(label2id)))

"""## Benchmarking"""

from sklearn.metrics import accuracy_score, f1_score

def compute_metrics_eval(pred):
  labels = pred.label_ids
  preds = pred.predictions.argmax(-1)
  f1 = f1_score(labels, preds, average='weighted')
  acc = accuracy_score(labels, preds)
  return {'accuracy': acc, 'f1': f1}

model_dict = {
    "bert-base" : "bert-base-uncased",
    "distilbert" : "distilbert-base-uncased",
    "mobilebert" : "google/mobilebert-uncased",
    "tinybert" : "huawei-noah/TinyBERT_General_4L_312D"
}

def train_model(model_name):
  model_ckpt = model_dict[model_name]
  tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
  config = AutoConfig.from_pretrained(model_ckpt, num_labels=num_labels, label2id=label2id, id2label=id2label)
  model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config).to(device)

  def local_tokenizer(batch):
    temp = tokenizer(batch['title'], padding=True, truncation=True)
    return temp

  encoded_dataset = dataset.map(local_tokenizer, batched=True, batch_size=None)

  trainer = Trainer(
      model=model,
      compute_metrics=compute_metrics_eval,
      args=training_args,
      train_dataset=encoded_dataset['train'],
      eval_dataset=encoded_dataset['validation'],
      tokenizer=tokenizer
  )
  trainer.train()
  preds_output = trainer.predict(encoded_dataset['test'])
  return preds_output.metrics


model_performance = {}
import time
for model_name in model_dict:
  print("\n\n")
  print(f"Model Name : {model_name}")
  start = time.time()
  result = train_model(model_name)
  end = time.time()
  model_performance[model_name] = {model_name : result, "Time Taken" : end-start}

model_performance

"""## Model Load & Save"""

trainer.save_model("Fake News")

from transformers import pipeline

classifier = pipeline("text-classification", model="Fake News")

classifier("Iran Supreme Leader Ayotollah Khemnini was assassinated.")

